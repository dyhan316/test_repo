{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## moving the data \n",
    "\n",
    "**이것을 이제 module화 시켜서, 다양한 tar file, self.glob에 대해서 해서 여러개의 file들을 평행하게 (using multiproceissing or sth) 만들어서 돌리기!)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os \n",
    "import shutil\n",
    "import glob \n",
    "import subprocess\n",
    "\n",
    "class Args():\n",
    "    def __init__(self):\n",
    "        self.T1_DWI_dir = \"/scratch/08834/tg881334/NDARINV0.tar\"\n",
    "        self.T2_dir = \"/scratch/08834/tg881334/NDARINV0-5.tar\"\n",
    "        self.save_dir = \"/scratch/08834/tg881334/2019\"\n",
    "        self.glob = \"NDARINV0BL\" #\"NDARINV0\" #let's look at this particular case (이러면 원래 너무 많을 수도 있따 )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOT <TarInfo 'NDARINV0' at 0x2b375427be80>\n",
      "/scratch/08834/tg881334/NDARINV0.tar\n"
     ]
    }
   ],
   "source": [
    "args = Args()\n",
    "file_to_untar = []\n",
    "\n",
    "\n",
    "last_member_names = []\n",
    "for tar_dir in [args.T1_DWI_dir]: #[args.T1_DWI_dir]:#[args.T2_dir, args.T1_DWI_dir]:\n",
    "    tar = tarfile.open(tar_dir)\n",
    "    for i,member in enumerate(tar):\n",
    "        file_name = member.name.split('/')[-1]\n",
    "        if not member.isfile(): #skip if not file\n",
    "            print(\"NOT\", member)\n",
    "            continue \n",
    "        elif member.name.split('_')[-3] != \"baselineYear1Arm1\": #skip if baseline\n",
    "            continue \n",
    "        elif args.glob not in file_name: #skip if doesn't match glob \n",
    "            continue \n",
    "        \n",
    "        file_to_untar.append(member)\n",
    "    \n",
    "        if i > 100000: #임의의 제한.. 굳이 둘 필요 없음!\n",
    "            break  \n",
    "    last_member_names.append(member)\n",
    "    print(tar_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<TarInfo 'NDARINV0/NDARINV00U4FTRU_baselineYear1Arm1_ABCD-MPROC-T1_20180519105135.tgz' at 0x2b3755ba6080>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_member_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<TarInfo 'NDARINV0/NDARINV0BL9EL2Y_baselineYear1Arm1_ABCD-MPROC-DTI_20180213110119.tgz' at 0x2b37544a2f80>, <TarInfo 'NDARINV0/NDARINV0BL9EL2Y_baselineYear1Arm1_ABCD-MPROC-T1_20180213103943.tgz' at 0x2b3755b67940>]\n"
     ]
    }
   ],
   "source": [
    "print(file_to_untar)\n",
    "tar.extractall(members = file_to_untar, path = args.save_dir) #extract files\n",
    "\n",
    "\n",
    "for i,member in enumerate(last_member_names):\n",
    "    #raise ValueError(\"muyst be changed\")\n",
    "    upper_dir = os.path.split(member.name)[0] #get uppder dir name\n",
    "    \n",
    "    [shutil.move(file, args.save_dir) for file in glob.glob(os.path.join(args.save_dir, f\"{upper_dir}/*\"))] #move them to the actual directory I wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDARINV0\n",
      "NDARINV0BL9EL2Y_baselineYear1Arm1_ABCD-MPROC-T1_20180213103943.tgz\n",
      "sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/anat/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_T1w.nii\n",
      "dataset_description.json\n",
      "sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/anat/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_T1w.json\n",
      "NDARINV0BL9EL2Y_baselineYear1Arm1_ABCD-MPROC-DTI_20180213110119.tgz\n",
      "sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/dwi/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_dwi.nii\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/anat/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_T1w.json: implausibly old time stamp 1969-12-31 18:00:00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_description.json\n",
      "sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/dwi/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_dwi.bval\n",
      "sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/dwi/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_dwi.bvec\n",
      "sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/dwi/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_dwi.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/dwi/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_dwi.bval: implausibly old time stamp 1969-12-31 18:00:00\n",
      "tar: sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/dwi/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_dwi.bvec: implausibly old time stamp 1969-12-31 18:00:00\n",
      "tar: sub-NDARINV0BL9EL2Y/ses-baselineYear1Arm1/dwi/sub-NDARINV0BL9EL2Y_ses-baselineYear1Arm1_run-01_dwi.json: implausibly old time stamp 1969-12-31 18:00:00\n"
     ]
    }
   ],
   "source": [
    "os.chdir(args.save_dir)\n",
    "\n",
    "#unzip the .tgz files\n",
    "for file in os.listdir(args.save_dir):\n",
    "    print(file)\n",
    "    if \"tgz\" in file: #i.e. if tgz\n",
    "        subprocess.run(f'tar zxvf {file}', shell = True)\n",
    "        subprocess.run(f'rm {file}', shell = True)\n",
    "    elif not \"sub-\" in file and \"dataset_description.json\" != file: #i.e. 이미 unzip된 sub-파일이 아니라면, 또는 dataset_description이 아니라면 \n",
    "        shutil.rmtree(file) #remove the nuisance stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "hi",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: hi"
     ]
    }
   ],
   "source": [
    "raise ValueError(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import pdb\n",
    "from pathlib import Path\n",
    "\n",
    "T2_dir = Path(\"/work2/08834/tg881334/stampede2/ABCD_preproc/txt_files/T2/\")\n",
    "T1_DWI_dir = Path(\"/work2/08834/tg881334/stampede2/ABCD_preproc/txt_files/T1_DWI/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(T2_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(T2_dir / \"dataset_collection.txt\", sep = \"\\t\")\n",
    "pd.read_csv(T2_dir / \"guid_pseudoguid.txt\", sep = \"\\t\")\n",
    "pd.read_csv(T2_dir / \"md5_values.txt\", sep = \"\\t\")\n",
    "pd.read_csv(T2_dir / \"fmriresults01.txt\", sep = \"\\t\") #this is what should be used\n",
    "pd.read_csv(T2_dir / \"package_info.txt\", sep = \"\\t\") #this is what should be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. actually reading the thing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "T1_DWI_meta = pd.read_csv(T1_DWI_dir / \"fmriresults01.txt\", sep = \"\\t\", header = 0 )\n",
    "T2_meta = pd.read_csv(T2_dir / \"fmriresults01.txt\", sep = \"\\t\", header = 0 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##removing the first layer because they won't be used (just extra descriptions)\n",
    "descriptions = T2_meta.iloc[0]\n",
    "T2_meta = T2_meta.drop(index = 0)\n",
    "T1_DWI_meta = T1_DWI_meta.drop(index=0)\n",
    "\n",
    "\n",
    "##seperating T1_DWI_meta into T1 and DWI \n",
    "T1_meta = T1_DWI_meta[T1_DWI_meta.scan_type == 'MR structural (T1)']\n",
    "DWI_meta = T1_DWI_meta[T1_DWI_meta.scan_type == 'multi-shell DTI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##just testing\n",
    "\n",
    "print(descriptions[0:5].values)\n",
    "print(T1_meta.shape, T2_meta.shape, DWI_meta.shape)\n",
    "T1_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##참조\n",
    "#check/proof that we're only dealing with QC-ed subjects\n",
    "(T1_meta.qc_outcome != \"pass\").sum() + (T2_meta.qc_outcome != \"pass\").sum() + (DWI_meta.qc_outcome != \"pass\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Merging the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Finding common subjects (which will be used)\n",
    "\n",
    "# 잠만 : step0에서 그 release별로 구분하는 것도 해야함! (우리는 releae4 만 할것...)\n",
    "# 따라서 밑에서 overlapping하는 사람들만 빼내는 것도, release4만 빼낸 후에 overlapping을 빼야할듯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##defining the subject sets\n",
    "T2_sub_set = set((T2_meta['subjectkey']).values)\n",
    "T1_sub_set = set((T1_meta['subjectkey']).values)\n",
    "DWI_sub_set = set((DWI_meta['subjectkey']).values)\n",
    "print(len(T1_sub_set), len(T2_sub_set), len(DWI_sub_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##getting intersection\n",
    "total_set = (T2_sub_set.intersection(T1_sub_set)).intersection(DWI_sub_set)\n",
    "print(len(total_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_list = [T1_meta, T2_meta, DWI_meta]\n",
    "\n",
    "meta = meta_list[0]\n",
    "\n",
    "haha = meta.loc[meta['subjectkey'].isin(total_set)]\n",
    "len(haha)\n",
    "#for meta in meta_list:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(T1_DWI_meta.scan_type.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_meta = T1_DWI_meta[T1_DWI_meta.scan_type == 'MR structural (T1)']\n",
    "DWI_meta = T1_DWI_meta[T1_DWI_meta.scan_type == 'multi-shell DTI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T2_meta.head()\n",
    "T1_DWI_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#T1_DWI_meta.sort_values('subjectkey')\n",
    "T1_DWI_sub_set = set((T1_DWI_meta['subjectkey'][1:]).values)\n",
    "T2_sub_set = set((T2_meta['subjectkey'][1:]).values)\n",
    "\n",
    "common = T1_DWI_sub_set.intersection(T2_sub_set)\n",
    "T1_only = T1_DWI_sub_set.intersection(T2_sub_set)\n",
    "T2_only = \n",
    "len(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_meta.index #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_meta.interview_age.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_meta[(T2_meta['subjectkey'] != T2_meta['src_subject_id'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_DWI_meta[(T1_DWI_meta['subjectkey'] != T1_DWI_meta['src_subject_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_meta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2_meta['subjectkey'][::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T1_DWI_meta['subjectkey'][::1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(T2_meta['qc_outcome'] == \"pass\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(T2_meta['qc_outcome'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CHA_preproc",
   "language": "python",
   "name": "cha_preproc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
